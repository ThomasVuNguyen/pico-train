# High Quality Training Config - Optimized for superior model performance
# This configuration prioritizes model quality over training speed
# Designed for RTX 5090 with focus on preventing overfitting and maximizing generalization

checkpointing:
  run_name: "pico-decoder-tiny-dolma29k-v3"
  save_to_hf: true
  hf_checkpoint:
    repo_id: "ThomasTheMaker/pico-decoder-tiny"
  save_every_n_steps: 500  # Frequent checkpoints for quality monitoring

  learning_dynamics:
    batch_size: 1  # Minimal batch size for learning dynamics
    eval_data: null  # Disable learning dynamics to save memory

model:
    d_model: 96
    activation_hidden_dim: 384
    dropout: 0.15  # Increased dropout for stronger regularization
    attention_dropout: 0.15  # Increased attention dropout
    layer_norm_eps: 1e-5  # Tighter normalization for stability

monitoring:
  save_to_wandb: false
  wandb:
    project: "pico-decoder-tiny"
    entity: "boymyc"
  logging:
    log_every_n_steps: 25  # Very frequent logging for quality monitoring

training:
  max_steps: 20000  # Longer training for better convergence
  optimization:
    lr: 0.00005  # Even lower learning rate for precision training
    lr_warmup_steps: 8000  # Extended warmup for stability
    lr_scheduler: "linear_with_warmup"  # Stable scheduler
    weight_decay: 0.02  # Increased weight decay for stronger regularization
    max_grad_norm: 0.5  # Tighter gradient clipping for stability
    gradient_accumulation_steps: 4  # Increased for better gradient estimates
    optimizer: "adamw"
    adam_beta1: 0.9  # Standard AdamW beta1
    adam_beta2: 0.999  # Standard AdamW beta2
    adam_epsilon: 1e-8  # Tighter epsilon for numerical stability
  
  fabric:
    num_nodes: 1
    num_devices: 1
    precision: "bf16-mixed"  # BF16 for Tensor Core optimization
  
evaluation: 
  paloma:
    batch_size: 1  # Minimal evaluation batch size
    eval_every_n_steps: 250  # Very frequent evaluation for quality monitoring
  
data:
  dataset:
    name: "pico-lm/pretokenized-dolma"
  dataloader:
    batch_size: 4  # Reduced for more stable training
  tokenizer:
    name: "allenai/OLMo-7B-0724-hf"
    vocab_size: 50304

# Quality-focused training strategy:
# 1. Lower learning rate (0.00005) for precise parameter updates
# 2. Extended warmup (8000 steps) for stable foundation
# 3. Stronger regularization (dropout 0.15, weight decay 0.02)
# 4. Tighter gradient clipping (0.5) for stability
# 5. More frequent evaluation (every 250 steps) for quality monitoring
# 6. Longer training (20000 steps) for full convergence
