# Max VRAM efficient config - uses as much VRAM as possible efficiently
# This configuration maximizes GPU utilization while staying within memory limits
# on GTX5090: optimized for Tensor Cores
# 
# If you still get OOM errors, run this before training:
# export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

checkpointing:
  run_name: "pico-decoder-tiny-dolma29k-v2"
  save_to_hf: true
  hf_checkpoint:
    repo_id: "ThomasTheMaker/pico-decoder-tiny"

  learning_dynamics:
    batch_size: 1  # Minimal batch size for learning dynamics
    eval_data: null  # Disable learning dynamics to save memory

model:
    d_model: 96
    activation_hidden_dim: 384
    dropout: 0.1  # Add dropout for regularization
    attention_dropout: 0.1  # Add attention dropout

monitoring:
  save_to_wandb: false
  wandb:
    project: "pico-decoder-tiny"
    entity: "boymyc"
  logging:
    log_every_n_steps: 50  # More frequent logging for better monitoring

training:
  optimization:
    lr: 0.0001  # Reduced from 3e-4 to prevent overfitting
    lr_warmup_steps: 5000  # Extended warmup for stability
    lr_scheduler: "linear_with_warmup"  # Supported scheduler with extended warmup
    weight_decay: 0.01  # Add weight decay for regularization
    max_grad_norm: 1.0  # Gradient clipping to prevent instability
    gradient_accumulation_steps: 2  # Reduced to work with larger batch size
  
  fabric:
    num_nodes: 1
    num_devices: 1
    precision: "bf16-mixed"  # BF16 for better Tensor Core utilization
  
evaluation: 
  paloma:
    batch_size: 1  # Minimal evaluation batch size
  
data:
  dataset:
    name: "pico-lm/pretokenized-dolma"
  dataloader:
    batch_size: 8  # Increased from 4 for better training quality
  tokenizer:
    name: "allenai/OLMo-7B-0724-hf"
    vocab_size: 50304
