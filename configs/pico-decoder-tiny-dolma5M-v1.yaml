# High Quality Training Config - Optimized for superior model performance
# This configuration prioritizes model quality over training speed
# Designed for RTX 5090 with focus on preventing overfitting and maximizing generalization
# Updated for scaling training on Dolma 5M tokens with stability-focused hyperparameters

checkpointing:
  run_name: "pico-decoder-tiny-dolma5M-v1"
  save_to_hf: true
  hf_checkpoint:
    repo_id: "ThomasTheMaker/pico-decoder-tiny"
  save_every_n_steps: 500  # Frequent checkpoints for quality monitoring

  learning_dynamics:
    batch_size: 1  # Minimal batch size for learning dynamics
    eval_data: null  # Disable learning dynamics to save memory

model:
    d_model: 96
    activation_hidden_dim: 384
    dropout: 0.15  # Increased dropout for stronger regularization
    attention_dropout: 0.15  # Increased attention dropout
    layer_norm_eps: 1e-5  # Tighter normalization for stability
    weight_init_type: "truncated_normal"  # Truncated normal for stability
    layer_norm_type: "rms_norm"  # RMSNorm for better stability
    use_qk_norm: true  # Query-Key normalization for attention stability

monitoring:
  save_to_wandb: false
  wandb:
    project: "pico-decoder-tiny"
    entity: "boymyc"
  logging:
    log_every_n_steps: 25  # Very frequent logging for quality monitoring

training:
  max_steps: 100000  # Longer training for better convergence
  optimization:
    lr: 0.00005  # Even lower learning rate for precision training
    lr_warmup_steps: 8000  # Extended warmup for stability
    lr_scheduler: "cosine"  # Cosine decay over full dataset for sustained learning
    weight_decay: 0.02  # Increased weight decay for stronger regularization
    max_grad_norm: 0.5  # Tighter gradient clipping for stability
    gradient_accumulation_steps: 4  # Increased for better gradient estimates
    optimizer: "adamw"
    adam_beta1: 0.9  # Standard AdamW beta1
    adam_beta2: 0.999  # Standard AdamW beta2
    adam_epsilon: 1e-8  # Tighter epsilon for numerical stability and convergence
  
  fabric:
    num_nodes: 1
    num_devices: 1
    precision: "bf16-mixed"  # BF16 for Tensor Core optimization
  
evaluation: 
  paloma:
    batch_size: 1  # Minimal evaluation batch size
    eval_every_n_steps: 250  # Very frequent evaluation for quality monitoring
  
data:
  dataset:
    name: "ThomasTheMaker/pretokenized-dolma-5M"  # Updated to 5M token dataset
  dataloader:
    batch_size: 4  # Reduced for more stable training
  tokenizer:
    name: "allenai/OLMo-7B-0724-hf"
    vocab_size: 50304

# Stability-focused training strategy for large-scale Dolma training:
# 1. Cosine learning rate schedule for sustained learning over full dataset
# 2. Truncated normal weight initialization to prevent extreme outliers
# 3. RMSNorm for better gradient stability during long training runs
# 4. Query-Key normalization (QK-Norm) to prevent attention logit overflow
# 5. AdamW epsilon 1e-8 for improved training stability and convergence
# 6. Extended warmup (8000 steps) for stable foundation
# 7. Stronger regularization (dropout 0.15, weight decay 0.02)
# 8. Tighter gradient clipping (0.5) for stability
# 9. More frequent evaluation (every 250 steps) for quality monitoring
# 10. Longer training (40000 steps) for full convergence on 5M tokens
